{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microtask0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Microtask 0: Use this notebook implementing the [Code_Changes metric](https://github.com/chaoss/wg-gmd/blob/master/implementations/Code_Changes-Git.ipynb) ([see it in MyBinder](https://hub.mybinder.org/user/chaoss-wg-gmd-lfs79xw9/notebooks/implementations/Code_Changes-Git.ipynb)) as an example of how to collect the data, producing a single JSON file per data source, with all items (commits, issues, pull/merge requests) in it. Produce one notebook per data source (git, GitHub/GitLab issues, GitHub pull requests / GitLab merge requests) showing a summary of the contents of that file (number of items in it, and number of different identities in it counting authors/committers for git, submitters for issues and pull/merge requests). This microtask is mandatory, to show that you can retrieve data and produde a notebook showing it. In each notebook, include also the list of repositories retrieved, and the date of retrieval, using data available in the JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have analyzed the [tensorflow/datasets](https://github.com/tensorflow/datasets) repository having 772 commits, around 150 issues(open + closed) and around 220 pull_requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceval\n",
    "Perceval is used to retreive the data - [link](https://chaoss.github.io/grimoirelab-tutorial/perceval/intro.html) to tutorial.\n",
    ">[Perceval](https://github.com/chaoss/grimoirelab-perceval) is a Python module for retrieving data from repositories related to software development. It works with many data sources, from git repositories and GitHub projects to mailing lists, Gerrit or StackOverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from access_token import ACCESS_TOKEN\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "owner, repo = 'tensorflow', 'datasets'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Perceval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Perceval **Git backend** for git repositories to retrieve data about commits, and **GitHub backend** to retrieve data about issues and pull requests using the [Github API](https://developer.github.com/).\n",
    "\n",
    "Perceval can be used:\n",
    "- As a python module - [git-module documentation](https://perceval.readthedocs.io/en/latest/perceval.backends.core.html#module-perceval.backends.core.git) and [github-module documentation](https://perceval.readthedocs.io/en/latest/perceval.backends.core.html#module-perceval.backends.core.github).\n",
    "- As a program from commandline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github access token is required for authenticated access to the Github API to extend the Api request rate limit.\n",
    "[Generating the access token](https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using from commandline - Retrieving data for the tensorflow/datasets repository and storing as json.\n",
    "(Removed the output from here after getting the tf_analysis.json file and converted the cell to markdown so the notebook doesn't look cluttered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   !perceval git --json-line https://github.com/tensorflow/datasets > tf_analysis.json\n",
    "#    !perceval github -t $ACCESS_TOKEN --json-line --sleep-for-rate --category issue $owner $repo >> tf_analysis.json\n",
    "#    !perceval github -t $ACCESS_TOKEN --json-line --sleep-for-rate --category pull_request $owner $repo >> tf_analysis.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the date and time of data retrieval using the retrieved data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date and time of data retrieval :  2019-03-24 01:58:23 UTC \n"
     ]
    }
   ],
   "source": [
    "with open('tf_analysis.json') as f:\n",
    "    line = f.readline()\n",
    "    line = json.loads(line) #convert string to dict\n",
    "time = datetime.datetime.utcfromtimestamp(line['timestamp']).strftime('%Y-%m-%d %H:%M:%S UTC %Z%z')\n",
    "print ('The date and time of data retrieval : ',time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datetime python module is used everywhere while working with strings, date and datetime objects - its documentation can be found [here](https://docs.python.org/3/library/datetime.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The further notebook is divided in three parts : Commits, Pull_requests and Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Commits \n",
    "### Class to summarize commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code_Changes:\n",
    "    \"\"\"Class for Code_Changes for Git repositories.\n",
    "    \n",
    "    Objects are instantiated by specifying a file with the\n",
    "    commits obtained by Perceval from a set of repositories.\n",
    "        \n",
    "    :param path: Path to file with one Perceval JSON document per line\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _summary(repo, cdata):\n",
    "        \"\"\"Compute a summary of a commit, suitable as a row in a dataframe\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            'repo': repo,\n",
    "            'hash': cdata['commit'],\n",
    "            'author': cdata['Author'],\n",
    "            'author_date': datetime.datetime.strptime(cdata['AuthorDate'],\n",
    "                                                      \"%a %b %d %H:%M:%S %Y %z\"),\n",
    "            'commit': cdata['Commit'],\n",
    "            'commit_date': datetime.datetime.strptime(cdata['CommitDate'],\n",
    "                                                      \"%a %b %d %H:%M:%S %Y %z\"),\n",
    "            'files_no': len(cdata['files'])\n",
    "        }\n",
    "        actions = 0\n",
    "        for file in cdata['files']:\n",
    "            if 'action' in file:\n",
    "                actions += 1\n",
    "        summary['files_action'] = actions\n",
    "        if 'Merge' in cdata:\n",
    "            summary['merge'] = True\n",
    "        else:\n",
    "            summary['merge'] = False\n",
    "        return summary;\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"Initilizes self.df, the dataframe with one row per commit.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = pd.DataFrame(columns=['hash', 'author', 'author_date',\n",
    "                                        'commit', 'commit_date',\n",
    "                                        'files_no', 'files_action',\n",
    "                                        'merge','repo'])\n",
    "        commits = []\n",
    "        with open(path) as commits_file:\n",
    "            for line in commits_file:\n",
    "                line = json.loads(line)\n",
    "                if(line['category']==\"commit\"):\n",
    "                    commit = line\n",
    "                    commits.append(self._summary(repo=commit['origin'],cdata=commit['data']))\n",
    "        \n",
    "        self.df = self.df.append(commits, sort=False)\n",
    "        self.df['author_date'] = pd.to_datetime(self.df['author_date'], utc=True)\n",
    "        self.df['commit_date'] = pd.to_datetime(self.df['commit_date'], utc=True)\n",
    "        \n",
    "    def total_count(self):\n",
    "        \n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def count(self, since = None, until = None, empty=True, merge=True, date='author_date'):\n",
    "        \"\"\"Count number of commits\n",
    "        \n",
    "        :param since: Period start\n",
    "        :param until: Period end\n",
    "        :param empty: Include empty commits\n",
    "        :param merge: Include merge commits\n",
    "        :param  date: Kind of date ('author_date' or 'commit_date')\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.df\n",
    "        if since:\n",
    "            df = df[df[date] >= since]\n",
    "        if until:\n",
    "            df = df[df[date] < until]\n",
    "        if not empty:\n",
    "            df = df[df['files_action'] != 0]\n",
    "        if not merge:\n",
    "            df = df[df['merge'] == False]\n",
    "        return df['hash'].nunique()\n",
    "    \n",
    "    def by_month(self):\n",
    "        \n",
    "        return self.df['author_date'] \\\n",
    "            .groupby([self.df.author_date.dt.year.rename('year'),\n",
    "                      self.df.author_date.dt.month.rename('month')]) \\\n",
    "            .agg('count')\n",
    "            \n",
    "    def unique_users(self):\n",
    "        return self.df.author.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Code_changes class to print some commit statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code changes total count: 696\n",
      "Code changes count all period: 696\n",
      "Code changes count from 2018-09-12 to 2019-03-10: 617\n",
      "Code changes count from 2018-09-12 to 2019-03-10:(no merge commits): 600\n",
      "Code changes count from 2018-09-12 to 2019-03-10:(no empty commits): 614\n"
     ]
    }
   ],
   "source": [
    "changes = Code_Changes('tf_analysis.json')\n",
    "print(\"Code changes total count:\", changes.total_count())\n",
    "print(\"Code changes count all period:\", changes.count())\n",
    "print(\"Code changes count from 2018-09-12 to 2019-03-10:\",\n",
    "      changes.count(since=\"2018-09-12\", until=\"2019-03-10\"))\n",
    "print(\"Code changes count from 2018-09-12 to 2019-03-10:(no merge commits):\",\n",
    "      changes.count(since=\"2018-09-12\", until=\"2019-03-10\", merge=False))\n",
    "print(\"Code changes count from 2018-09-12 to 2019-03-10:(no empty commits):\",\n",
    "      changes.count(since=\"2018-09-12\", until=\"2019-03-10\", empty=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year  month\n",
       "2018  9         10\n",
       "      10        28\n",
       "      11       107\n",
       "      12       137\n",
       "2019  1        122\n",
       "      2        148\n",
       "      3        144\n",
       "Name: author_date, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes.by_month()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The commits dataframe created by the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash</th>\n",
       "      <th>author</th>\n",
       "      <th>author_date</th>\n",
       "      <th>commit</th>\n",
       "      <th>commit_date</th>\n",
       "      <th>files_no</th>\n",
       "      <th>files_action</th>\n",
       "      <th>merge</th>\n",
       "      <th>repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>680c6bda1b7d6db9d74f9c1373ebe737938926a7</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-12 21:23:17+00:00</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-12 21:23:17+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/tensorflow/datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87fd7f9fc4e36aa917726f3e8f89b7c89c99fb66</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-11 19:10:03+00:00</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-12 21:28:48+00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/tensorflow/datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fb0f20383bb2a83477770517767b53ad97ec0840</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-12 20:12:22+00:00</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-12 21:28:57+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/tensorflow/datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca9ffcccb2ae3409c4210475b6407b871ad51ba9</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-13 19:53:54+00:00</td>\n",
       "      <td>Ryan Sepassi &lt;rsepassi@google.com&gt;</td>\n",
       "      <td>2018-09-13 19:59:39+00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/tensorflow/datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3f650ac05ea4e5d1eeef55a9d3a70a75b1cb5843</td>\n",
       "      <td>Dustin Tran &lt;trandustin@google.com&gt;</td>\n",
       "      <td>2018-09-20 18:59:48+00:00</td>\n",
       "      <td>Copybara-Service &lt;copybara-piper@google.com&gt;</td>\n",
       "      <td>2018-09-21 23:28:12+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>https://github.com/tensorflow/datasets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       hash  \\\n",
       "0  680c6bda1b7d6db9d74f9c1373ebe737938926a7   \n",
       "1  87fd7f9fc4e36aa917726f3e8f89b7c89c99fb66   \n",
       "2  fb0f20383bb2a83477770517767b53ad97ec0840   \n",
       "3  ca9ffcccb2ae3409c4210475b6407b871ad51ba9   \n",
       "4  3f650ac05ea4e5d1eeef55a9d3a70a75b1cb5843   \n",
       "\n",
       "                                author               author_date  \\\n",
       "0   Ryan Sepassi <rsepassi@google.com> 2018-09-12 21:23:17+00:00   \n",
       "1   Ryan Sepassi <rsepassi@google.com> 2018-09-11 19:10:03+00:00   \n",
       "2   Ryan Sepassi <rsepassi@google.com> 2018-09-12 20:12:22+00:00   \n",
       "3   Ryan Sepassi <rsepassi@google.com> 2018-09-13 19:53:54+00:00   \n",
       "4  Dustin Tran <trandustin@google.com> 2018-09-20 18:59:48+00:00   \n",
       "\n",
       "                                         commit               commit_date  \\\n",
       "0            Ryan Sepassi <rsepassi@google.com> 2018-09-12 21:23:17+00:00   \n",
       "1            Ryan Sepassi <rsepassi@google.com> 2018-09-12 21:28:48+00:00   \n",
       "2            Ryan Sepassi <rsepassi@google.com> 2018-09-12 21:28:57+00:00   \n",
       "3            Ryan Sepassi <rsepassi@google.com> 2018-09-13 19:59:39+00:00   \n",
       "4  Copybara-Service <copybara-piper@google.com> 2018-09-21 23:28:12+00:00   \n",
       "\n",
       "  files_no files_action  merge                                    repo  \n",
       "0        1            1  False  https://github.com/tensorflow/datasets  \n",
       "1       11           11  False  https://github.com/tensorflow/datasets  \n",
       "2        2            2  False  https://github.com/tensorflow/datasets  \n",
       "3        5            5  False  https://github.com/tensorflow/datasets  \n",
       "4        2            2  False  https://github.com/tensorflow/datasets  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of different users involved in the commits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different identities :  42\n"
     ]
    }
   ],
   "source": [
    "print('Number of different identities : ',changes.unique_users()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of commits that make changes in the master branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Commits (master branch): 677\n"
     ]
    }
   ],
   "source": [
    "# Create a dict having all commits\n",
    "commits = {}\n",
    "with open('tf_analysis.json') as commits_file:\n",
    "    for line in commits_file:\n",
    "        line = json.loads(line)\n",
    "        if(line['category']==\"commit\"):\n",
    "            commit = line\n",
    "            commits[commit['data']['commit']] = commit\n",
    "            \n",
    "# Find commits in master branch.\n",
    "# Start by adding head to an empty todo list. Then loop until todo set is empty:\n",
    "# for each commit in the todo list, add it to the master set, and go backwards\n",
    "# (finding parents), adding them to the todo set.\n",
    "todo = set()\n",
    "for id, commit in commits.items():\n",
    "    if 'HEAD -> refs/heads/master' in commit['data']['refs']:\n",
    "        todo.add(id)\n",
    "\n",
    "        \n",
    "master = set()\n",
    "while len(todo) > 0:\n",
    "    current = todo.pop()\n",
    "    master.add(current)\n",
    "    for parent in commits[current]['data']['parents']:\n",
    "        if parent not in master:\n",
    "            todo.add(parent)\n",
    "    \n",
    "code_commits = len(master)\n",
    "    \n",
    "print(\"Code Commits (master branch):\", code_commits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Pull_requests\n",
    "### Visualizing the json structure of a pull_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tf_analysis.json') as pr_file:\n",
    "            for line in pr_file:\n",
    "                line = json.loads(line)\n",
    "                if(line['category']==\"issue\"):\n",
    "                    pr = line\n",
    "                    break\n",
    "pprint(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to summarize pull_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pr_statistics:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _summary(pr_data):\n",
    "        \"\"\"Compute a summary of a pull_request, suitable as a row in a dataframe\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            'base_repo': pr_data['base']['label'],\n",
    "            'title': pr_data['title'],\n",
    "            'state': pr_data['state'],\n",
    "            'commits':pr_data['commits'],\n",
    "            'commits_data': pr_data['commits_data'],\n",
    "            'comments':pr_data['comments'],\n",
    "            'changed_files': pr_data['changed_files'],\n",
    "            'additions': pr_data['additions'],\n",
    "            'deletions': pr_data['deletions'],\n",
    "            'created_at': pr_data['created_at'],\n",
    "            'closed_at': pr_data['closed_at'],\n",
    "            'user': pr_data['user_data']['login']\n",
    "        }\n",
    "        if (pr_data['merged']):\n",
    "            summary['merged_at'] = pr_data['merged_at']\n",
    "        else:\n",
    "            summary['merged_at'] = None\n",
    "        return summary;\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "           Initilizes self.df, the dataframe with one row per pull_request.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = pd.DataFrame(columns=['base_repo','title', 'state', 'commits', 'commits_data',\n",
    "                                        'comments', 'changed_files', 'additions', 'deletions', 'created_at'\n",
    "                                        'closed_at','user', 'merged_at'])\n",
    "        pull_requests= []\n",
    "        with open(path) as pr_file:\n",
    "            for line in pr_file:\n",
    "                line = json.loads(line)\n",
    "                if(line['category']==\"pull_request\"):\n",
    "                    pr = line\n",
    "                    pull_requests.append(self._summary(pr_data=pr['data']))\n",
    "\n",
    "        self.df = self.df.append(pull_requests, sort=False)\n",
    "        #self.df['author_date'] = pd.to_datetime(self.df['author_date'], utc=True)\n",
    "        #self.df['commit_date'] = pd.to_datetime(self.df['commit_date'], utc=True)\n",
    "        \n",
    "    def total_count(self):\n",
    "        \n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def open_prs(self):\n",
    "        return len(self.df.index[self.df['state']=='open'])\n",
    "    \n",
    "    def closed_prs(self):\n",
    "        return len(self.df.index[self.df['state']=='closed'])\n",
    "    \n",
    "    def unique_users(self):\n",
    "        return self.df.user.nunique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique contributors for pull_requests, and number of open and closed pull_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of open prs 60\n",
      "Number of closed prs 117\n",
      "Number of unique users 36\n"
     ]
    }
   ],
   "source": [
    "pull_reqs = pr_statistics('./tf_analysis.json')\n",
    "print('Number of open prs', pull_reqs.open_prs())\n",
    "print('Number of closed prs', pull_reqs.closed_prs())\n",
    "print('Number of unique users', pull_reqs.unique_users())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Issues\n",
    "Note : It would contain pull_requests data also marked as category issue, since as in Github API, all pull_requests are issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class to summarize issues (similar to the pr_statistics class above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class issue_statistics:\n",
    "    \n",
    "    @staticmethod\n",
    "    def _summary(issue_data):\n",
    "        \"\"\"Compute a summary of a pull_request, suitable as a row in a dataframe\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            'title': issue_data['title'],\n",
    "            'state': issue_data['state'],\n",
    "            'comments':issue_data['comments'],\n",
    "            'created_at': issue_data['created_at'],\n",
    "            'closed_at': issue_data['closed_at'],\n",
    "            'created_by': issue_data['user_data']['login']\n",
    "        }\n",
    "        return summary;\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "           Initilizes self.df, the dataframe with one row per issue.\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = pd.DataFrame(columns=['title', 'state', 'created_at', 'closed_at', 'comments', 'created_by'])\n",
    "        issues = []\n",
    "        with open(path) as issue_file:\n",
    "            for line in issue_file:\n",
    "                line = json.loads(line)\n",
    "                if(line['category']==\"issue\"):\n",
    "                    issue = line\n",
    "                    issues.append(self._summary(issue_data=issue['data']))\n",
    "\n",
    "        self.df = self.df.append(issues, sort=False)\n",
    "        #self.df['author_date'] = pd.to_datetime(self.df['author_date'], utc=True)\n",
    "        #self.df['commit_date'] = pd.to_datetime(self.df['commit_date'], utc=True)\n",
    "        \n",
    "    def total_count(self):        \n",
    "        return len(self.df.index)\n",
    "    \n",
    "    def open_issues(self):\n",
    "        return len(self.df.index[self.df['state']=='open'])\n",
    "    \n",
    "    def closed_issues(self):\n",
    "        return len(self.df.index[self.df['state']=='closed'])\n",
    "    \n",
    "    def unique_users(self):\n",
    "        return self.df.created_by.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of unique issue contributors, and the numbere of open and closed issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of open issues 152\n",
      "Number of closed issues 171\n",
      "Number of unique issue creators 80\n"
     ]
    }
   ],
   "source": [
    "issues = issue_statistics('./tf_analysis.json'\n",
    "                         )\n",
    "print('Number of open issues', issues.open_issues())\n",
    "print('Number of closed issues', issues.closed_issues())\n",
    "print('Number of unique issue creators', issues.unique_users())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the issue dataframe created by this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>state</th>\n",
       "      <th>created_at</th>\n",
       "      <th>closed_at</th>\n",
       "      <th>comments</th>\n",
       "      <th>created_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Example in README not working</td>\n",
       "      <td>closed</td>\n",
       "      <td>2018-09-17T18:42:13Z</td>\n",
       "      <td>2018-09-17T21:56:55Z</td>\n",
       "      <td>2</td>\n",
       "      <td>piyush-kgp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fixes bug - datasets.load was not implemented</td>\n",
       "      <td>closed</td>\n",
       "      <td>2018-09-17T19:57:28Z</td>\n",
       "      <td>2018-09-17T21:56:03Z</td>\n",
       "      <td>1</td>\n",
       "      <td>piyush-kgp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>error on pip install</td>\n",
       "      <td>closed</td>\n",
       "      <td>2018-09-26T12:32:21Z</td>\n",
       "      <td>2018-10-01T07:16:31Z</td>\n",
       "      <td>1</td>\n",
       "      <td>tiaguinho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Import error on Windows</td>\n",
       "      <td>closed</td>\n",
       "      <td>2018-09-14T10:09:00Z</td>\n",
       "      <td>2018-10-01T07:17:00Z</td>\n",
       "      <td>3</td>\n",
       "      <td>LoSealL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Question] - Would it be okay to generate tfre...</td>\n",
       "      <td>closed</td>\n",
       "      <td>2018-11-28T19:33:58Z</td>\n",
       "      <td>2018-11-29T19:11:58Z</td>\n",
       "      <td>1</td>\n",
       "      <td>ksachdeva</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   state  \\\n",
       "0                      Example in README not working  closed   \n",
       "1      Fixes bug - datasets.load was not implemented  closed   \n",
       "2                               error on pip install  closed   \n",
       "3                            Import error on Windows  closed   \n",
       "4  [Question] - Would it be okay to generate tfre...  closed   \n",
       "\n",
       "             created_at             closed_at comments  created_by  \n",
       "0  2018-09-17T18:42:13Z  2018-09-17T21:56:55Z        2  piyush-kgp  \n",
       "1  2018-09-17T19:57:28Z  2018-09-17T21:56:03Z        1  piyush-kgp  \n",
       "2  2018-09-26T12:32:21Z  2018-10-01T07:16:31Z        1   tiaguinho  \n",
       "3  2018-09-14T10:09:00Z  2018-10-01T07:17:00Z        3     LoSealL  \n",
       "4  2018-11-28T19:33:58Z  2018-11-29T19:11:58Z        1   ksachdeva  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issues.df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
